
# 1. 游닍 Importar librer칤as
from pyspark.sql import SparkSession
from pyspark.sql.functions import *
import boto3

# 2. 游 Crear la SparkSession
spark = SparkSession.builder \
    .appName("ETL_Datos_Meteorologicos") \
    .config("spark.hadoop.fs.s3a.impl", "org.apache.hadoop.fs.s3a.S3AFileSystem") \
    .config("spark.hadoop.fs.s3a.aws.credentials.provider", "com.amazonaws.auth.DefaultAWSCredentialsProviderChain") \
    .getOrCreate()

# 3. 游댏 Configurar acceso a S3 (solo si usas credenciales locales)
spark._jsc.hadoopConfiguration().set("fs.s3a.access.key", "TU_ACCESS_KEY")
spark._jsc.hadoopConfiguration().set("fs.s3a.secret.key", "TU_SECRET_KEY")
spark._jsc.hadoopConfiguration().set("fs.s3a.endpoint", "s3.amazonaws.com")

# 4. 游닌 Cargar datos desde Amazon S3
ruta_origen = "s3a://mi-bucket/clima/"
df_raw = spark.read.option("header", True).csv(ruta_origen)

# 5. 游빍 Transformaci칩n de datos
df_limpio = df_raw.dropna().withColumn("temp_media", (col("temp_max") + col("temp_min"))/2)

# 6. 游댍 Validaci칩n de datos
df_limpio.printSchema()
df_limpio.show(5)

# 7. 游 Guardar datos procesados en formato Parquet
ruta_destino = ruta_origen.replace("raw", "processed")
df_limpio.write.mode("overwrite").parquet(ruta_destino)

# 8. 游빛 Cierre de sesi칩n Spark
spark.stop()
