# ğŸ§  Portafolio de Proyectos ETL con PySpark + AWS

Este repositorio contiene 5 proyectos tipo ETL desarrollados en PySpark, integrando servicios de Amazon Web Services (AWS) como Amazon S3, Redshift y mÃ¡s. Todos los proyectos estÃ¡n diseÃ±ados para ejecutarse en notebooks de Jupyter y pueden ser orquestados fÃ¡cilmente con Apache Airflow.

---

## ğŸ“ Estructura del repositorio

```bash
ETL-PYSPARK-AWS/
â”‚
â”œâ”€â”€ notebooks/
â”‚   â”œâ”€â”€ ETL_logs_web.ipynb
â”‚   â”œâ”€â”€ ETL_datos_meteorologicos.ipynb
â”‚   â”œâ”€â”€ ETL_tweets_sentimiento.ipynb
â”‚   â”œâ”€â”€ ETL_ventas_retail.ipynb
â”‚   â””â”€â”€ ETL_facturacion_electronica.ipynb
â”‚
â”œâ”€â”€ airflow/
â”‚   â””â”€â”€ dag_ejemplo.py
â”‚
â”œâ”€â”€ data/
â”‚   â””â”€â”€ ejemplo.csv  # (Opcional: muestra o link a dataset)
â”‚
â”œâ”€â”€ README.md
â”œâ”€â”€ requirements.txt
â””â”€â”€ .gitignore
```

---

## ğŸ“… Lista de proyectos

1. **ETL de Logs Web**
   - Fuente: Logs en texto plano en Amazon S3
   - TransformaciÃ³n: ExtracciÃ³n de cÃ³digos 404, limpieza de logs
   - Destino: Amazon S3 (Parquet)

2. **ETL de Datos MeteorolÃ³gicos**
   - Fuente: CSVs meteorolÃ³gicos en S3
   - TransformaciÃ³n: CÃ¡lculo de temperatura media y limpieza
   - Destino: Amazon S3 (Parquet)

3. **ETL de Tweets para AnÃ¡lisis de Sentimiento**
   - Fuente: JSONs desde la API de Twitter
   - TransformaciÃ³n: Limpieza de texto, anÃ¡lisis de sentimiento
   - Destino: Redshift o DynamoDB

4. **ETL de Ventas Retail**
   - Fuente: Datos CSV de puntos de venta (POS)
   - TransformaciÃ³n: CÃ¡lculo de total por venta
   - Destino: Amazon S3 o Redshift

5. **ETL de FacturaciÃ³n ElectrÃ³nica**
   - Fuente: Archivos JSON/XML de comprobantes electrÃ³nicos
   - TransformaciÃ³n: ExtracciÃ³n de RUC, monto y fecha
   - Destino: Aurora PostgreSQL o Amazon S3

---

## âš™ï¸ TecnologÃ­as usadas

- Apache Spark (PySpark)
- Jupyter Notebook
- Amazon S3
- Amazon Redshift / Aurora / DynamoDB
- AWS Glue Catalog
- Apache Airflow (para orquestaciÃ³n)

---

## ğŸš€ Instrucciones para ejecutar

1. Clonar este repositorio:
```bash
git clone https://github.com/Data-Engineer-FJ/ETL-PYSPARK-AWS.git
```

2. Instalar las dependencias:
```bash
pip install -r requirements.txt
```

3. Configurar tus credenciales de AWS (ej. `~/.aws/credentials` o variables de entorno).

4. Abrir los notebooks con Jupyter:
```bash
jupyter notebook
```

5. Ejecutar cada notebook paso a paso.

---

## ğŸ”„ OrquestaciÃ³n con Airflow (opcional)

Para ejecutar estos pipelines con Airflow:
1. Ir al directorio `airflow/`.
2. Configurar el DAG `dag_ejemplo.py` con el operador `PapermillOperator` o `SparkSubmitOperator`.
3. Iniciar el entorno Airflow y activar el DAG.

---

## ğŸ“Š Ejemplo de resultado

Incluye capturas de pantalla o enlaces a visualizaciones generadas con los datos procesados (QuickSight, Tableau, etc.)

---

## âœï¸ Contribuciones

Las contribuciones son bienvenidas. Por favor abre un issue o un pull request si deseas mejorar este repositorio.

---

## âœ‰ï¸ Licencia

Este proyecto estÃ¡ bajo la licencia MIT. Consulta el archivo `LICENSE` para mÃ¡s detalles.

---

## ğŸŒŸ Autor

Desarrollado por [Fredy Johel](https://github.com/Data-Engineer-FJ) | Data Engineer especializado en pipelines de datos, AWS y soluciones escalables.

