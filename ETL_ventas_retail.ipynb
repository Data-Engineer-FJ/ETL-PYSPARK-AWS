
# 1. 📦 Importar librerías
from pyspark.sql import SparkSession
from pyspark.sql.functions import *
import boto3

# 2. 🚀 Crear la SparkSession
spark = SparkSession.builder \
    .appName("ETL_Ventas_Retail") \
    .config("spark.hadoop.fs.s3a.impl", "org.apache.hadoop.fs.s3a.S3AFileSystem") \
    .config("spark.hadoop.fs.s3a.aws.credentials.provider", "com.amazonaws.auth.DefaultAWSCredentialsProviderChain") \
    .getOrCreate()

# 3. 🔐 Configurar acceso a S3 (solo si usas credenciales locales)
spark._jsc.hadoopConfiguration().set("fs.s3a.access.key", "TU_ACCESS_KEY")
spark._jsc.hadoopConfiguration().set("fs.s3a.secret.key", "TU_SECRET_KEY")
spark._jsc.hadoopConfiguration().set("fs.s3a.endpoint", "s3.amazonaws.com")

# 4. 📥 Cargar datos desde Amazon S3
ruta_origen = "s3a://mi-bucket/ventas/"
df_raw = spark.read.option("header", True).csv(ruta_origen)

# 5. 🧪 Transformación de datos
df_limpio = df_raw.dropna().withColumn("total", col("cantidad") * col("precio_unitario"))

# 6. 🔎 Validación de datos
df_limpio.printSchema()
df_limpio.show(5)

# 7. 💾 Guardar datos procesados en formato Parquet
ruta_destino = ruta_origen.replace("raw", "processed")
df_limpio.write.mode("overwrite").parquet(ruta_destino)

# 8. 🧹 Cierre de sesión Spark
spark.stop()
